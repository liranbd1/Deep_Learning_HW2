{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxp1xCEEAEcpm8Y+dbj5xH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liranbd1/Deep_Learning_HW2/blob/main/Deep_Learning_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXgjA6zUDF3o"
      },
      "source": [
        "**NOTE!!!**\n",
        "\n",
        "In this code there are hard coded paths to Google Drive which load the dataset, write files and create folder, please make sure to change the path parameters to avoid this if not wanted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2gZMMTHlFpw"
      },
      "source": [
        "import os\n",
        "import torchvision\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tkinter as tk\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Ev8KzTRobI"
      },
      "source": [
        "**Connecting to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19kiJwnJmwln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4ca60f-2645-4592-95d1-42f2fe866076"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46p9-08wqeAI"
      },
      "source": [
        "**Custom Dataset class**\n",
        "\n",
        "Custom dataset class to load image files with their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9UYiw3oqlYf"
      },
      "source": [
        "class GTSRB(Dataset):\n",
        "    def __init__(self, root, csv, transform = False, shuffle = False):\n",
        "        self.root_dirc = root\n",
        "        self.df = pd.read_csv(os.path.join(root, csv))\n",
        "        self.transform = transform\n",
        "        if shuffle:\n",
        "            self.df = self.df.sample(frac=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_row = self.df.iloc[index]\n",
        "        img_path = os.path.join(self.root_dirc, img_row['Path'])\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        label = img_row['ClassId']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_H8aHEnk_hV"
      },
      "source": [
        "**Loading Dataset**\n",
        "\n",
        "First block will be loading datasets from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbGgi9Jmk8v5"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((30,30)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.34, 0.31, 0.31),(0.27,0.26,0.26))\n",
        "    ]\n",
        ")\n",
        "\n",
        "def load_datasets():\n",
        "    data_path = os.path.join(root_path, 'datasets')\n",
        "    train_csv_filename = 'Train.csv'\n",
        "    test_csv_filename = 'Test.csv'\n",
        "\n",
        "    # Loading datasets \n",
        "    train_dataset = GTSRB(data_path, train_csv_filename, transform, shuffle = True)\n",
        "    test_dataset = GTSRB(data_path, test_csv_filename, transform)\n",
        "\n",
        "    # Splitting train to train and validation.\n",
        "    train_len = int(0.8*len(train_dataset))\n",
        "    val_len = len(train_dataset) - train_len\n",
        "\n",
        "    train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
        "\n",
        "    # Creating dataloaders\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=2)\n",
        "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64, num_workers=2)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=2)\n",
        "\n",
        "    return train_dataloader, validation_dataloader, test_dataloader"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfmyTaMq2lv"
      },
      "source": [
        "**Hyper Parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxUwQ7Cd4-d"
      },
      "source": [
        "# Setting the device to Cuda if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Output and input layers\n",
        "output_layer_size = 43\n",
        "input_layer_size = 3\n",
        "\n",
        "# Hyper parameters for NoDropNoBNCNN\n",
        "NoDropNoBN_kernel = 3\n",
        "NoDropNoBN_strid = 1\n",
        "NoDropNoBN_padding = 1\n",
        "\n",
        "# Early stopping parameter\n",
        "p = 20\n",
        "\n",
        "# Optimizers\n",
        "\n",
        "\n",
        "# Learning rates\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Paths\n",
        "root_path = './gdrive/MyDrive/Deep_Learning_HW2'\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoxpQ4auq6Ox"
      },
      "source": [
        "# DataLoaders\n",
        "train_dl, validation_dl, test_dl = load_datasets()\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cIXBeCUL5AM",
        "outputId": "fd543943-32e4-4b53-f3ab-c95d7f173914"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-l7xZsTvhEo"
      },
      "source": [
        "**Model 1**\n",
        "\n",
        "No dropout and no batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUInffkjvrEN"
      },
      "source": [
        "class NoDropNoBNCNN(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels=32, kernel_size=3, stride=1, padding= 1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Conv2d(32, out_channels=64, kernel_size=3, stride=1, padding= 1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Conv2d(64, 64, 3,stride=1, padding=1 ),\n",
        "      nn.MaxPool2d(2)\n",
        "    )\n",
        "    #Add another Convolution layer or reduce the number of feature maps\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(576, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, output_layer_size)\n",
        "    )\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.feature_extractor(x)\n",
        "    #print('x:', x.shape)\n",
        "    #print('features: ', features.shape)\n",
        "    class_scores = self.classifier(features)\n",
        "   \n",
        "    #print('class_scores: ', class_scores.shape)\n",
        "    y_pred = self.softmax(class_scores)\n",
        "    return y_pred"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE3FI95Y3-qe"
      },
      "source": [
        "def training(epochs, optimizer, model, loss_func, patience, model_type):\n",
        "  patience_count = 0\n",
        "  model.train()\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  best_model_path = os.path.join(root_path, f'best_model{model_type}.pth')\n",
        "  for i in range(epochs):\n",
        "    for idx, (data, label) in enumerate(train_dl):\n",
        "      print(f'data index {idx}')\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(data.to(device))\n",
        "      loss = loss_func(y_hat, label.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    # Calculate Accuracy for train and validation\n",
        "    train_acc_list.append(calculate_accuracy(train_dl, model))\n",
        "    val_acc_list.append(calculate_accuracy(validation_dl, model))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for j, (data, label) in enumerate(validation_data):\n",
        "          optimizer.zero_grad()\n",
        "          y_hat = model(data.to(device))\n",
        "          loss = loss_function(y_hat, label.to(device))\n",
        "          optimizer.step()\n",
        "      loss_val = loss.detach()\n",
        "      if loss_val < minimal_loss:\n",
        "        minimal_loss = loss_value\n",
        "        patience_count = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "      elif loss_val >= minimal_loss:\n",
        "        patience_count += 1\n",
        "    \n",
        "    if patience_count == patience:\n",
        "      break\n",
        "\n",
        "  return train_acc_list, val_acc_list\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-se1L4aYEOv"
      },
      "source": [
        "def calculate_accuracy(dataloader, model, title=None):\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  \n",
        "  for j, (data,label) in enumerate(data_loader):\n",
        "    y_hat = model(data)\n",
        "    predictions = torch.argmax(y_hat, dim=1)\n",
        "    correct_count += torch.sum(predictions == label.to(device)).type(torch.float32)\n",
        "    total_count += data.shape[0]\n",
        "  accuracy = (correct_count/ total_count).item()*100\n",
        "  \n",
        "  # This section is relevant to the test accuracy\n",
        "  if title != None:\n",
        "    # Checking if path exists and if not create it\n",
        "    if not os.path.exists('./gdrive/MyDrive/Deep_Learning_HW2/accuracy_file'):\n",
        "      try: # Since we can throw an error here we prefer to wrap it in try and catch and raise the error back to not continue the run\n",
        "        !mkdir './gdrive/MyDrive/Deep_Learning_HW2/accuracy_file'\n",
        "      except OSError as error:\n",
        "        print(error)\n",
        "        raise error \n",
        "    # Writing the accuracy to a file\n",
        "    with open(os.path.join('./gdrive/MyDrive/Deep_Learning_HW2/accuracy_file',f'{line_title}.txt'), 'w') as file:\n",
        "      file.write(f\"accuracy : {accuracy} %\")\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUrrLSLaaSWW"
      },
      "source": [
        "def plot_report(x_axis, title, y_axis):\n",
        "  \n",
        "  file_name = title\n",
        "  if not os.path.exists('./gdrive/MyDrive/Deep_Learning_HW2/plots'):\n",
        "      try: # Since we can throw an error here we prefer to wrap it in try and catch and raise the error back to not continue the run\n",
        "        !mkdir './gdrive/MyDrive/Deep_Learning_HW2/plots'\n",
        "      except OSError as error:\n",
        "        print(error)\n",
        "        raise error \n",
        "\n",
        "  fig = plt.figure()\n",
        "  plt.plot(x_axis, y_axis, 'b')\n",
        "  plt.title(title)\n",
        "  file_path = os.path.join('./gdrive/MyDrive/Deep_Learning_HW2/plots', f'{file_name}')\n",
        "  fig.savefig(file_path)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVRTSPiTcbQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbc707a-1330-4c91-def3-edacc4982a14"
      },
      "source": [
        "model = NoDropNoBNCNN(3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "train_acc, val_acc = training(epochs, optimizer, model, loss_function, 5, 'No_D_No_BN')\n",
        "\n",
        "plot_report(train_acc, \"No_D_No_BN train accuracy\", epochs)\n",
        "plot_report(val_acc, \"No_D_No_BN validation accuracy\", epochs)\n",
        "\n",
        "test_acc = calculate_accuracy(test_dl, model, 'No_d_No_BN test accuracy')\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data index 0\n",
            "data index 1\n",
            "data index 2\n",
            "data index 3\n",
            "data index 4\n",
            "data index 5\n",
            "data index 6\n",
            "data index 7\n",
            "data index 8\n",
            "data index 9\n",
            "data index 10\n",
            "data index 11\n",
            "data index 12\n",
            "data index 13\n",
            "data index 14\n",
            "data index 15\n",
            "data index 16\n",
            "data index 17\n",
            "data index 18\n",
            "data index 19\n",
            "data index 20\n",
            "data index 21\n",
            "data index 22\n",
            "data index 23\n",
            "data index 24\n",
            "data index 25\n",
            "data index 26\n",
            "data index 27\n",
            "data index 28\n",
            "data index 29\n",
            "data index 30\n",
            "data index 31\n",
            "data index 32\n",
            "data index 33\n",
            "data index 34\n",
            "data index 35\n",
            "data index 36\n",
            "data index 37\n",
            "data index 38\n",
            "data index 39\n",
            "data index 40\n",
            "data index 41\n",
            "data index 42\n",
            "data index 43\n",
            "data index 44\n",
            "data index 45\n",
            "data index 46\n",
            "data index 47\n",
            "data index 48\n",
            "data index 49\n",
            "data index 50\n",
            "data index 51\n",
            "data index 52\n",
            "data index 53\n",
            "data index 54\n",
            "data index 55\n",
            "data index 56\n",
            "data index 57\n",
            "data index 58\n",
            "data index 59\n",
            "data index 60\n",
            "data index 61\n",
            "data index 62\n",
            "data index 63\n",
            "data index 64\n",
            "data index 65\n",
            "data index 66\n",
            "data index 67\n",
            "data index 68\n",
            "data index 69\n",
            "data index 70\n",
            "data index 71\n",
            "data index 72\n",
            "data index 73\n",
            "data index 74\n",
            "data index 75\n",
            "data index 76\n",
            "data index 77\n",
            "data index 78\n",
            "data index 79\n",
            "data index 80\n",
            "data index 81\n",
            "data index 82\n",
            "data index 83\n",
            "data index 84\n",
            "data index 85\n",
            "data index 86\n",
            "data index 87\n",
            "data index 88\n",
            "data index 89\n",
            "data index 90\n",
            "data index 91\n",
            "data index 92\n",
            "data index 93\n",
            "data index 94\n",
            "data index 95\n",
            "data index 96\n",
            "data index 97\n",
            "data index 98\n",
            "data index 99\n",
            "data index 100\n",
            "data index 101\n",
            "data index 102\n",
            "data index 103\n",
            "data index 104\n",
            "data index 105\n",
            "data index 106\n",
            "data index 107\n",
            "data index 108\n",
            "data index 109\n",
            "data index 110\n",
            "data index 111\n",
            "data index 112\n",
            "data index 113\n",
            "data index 114\n",
            "data index 115\n",
            "data index 116\n",
            "data index 117\n",
            "data index 118\n",
            "data index 119\n",
            "data index 120\n",
            "data index 121\n",
            "data index 122\n",
            "data index 123\n",
            "data index 124\n",
            "data index 125\n",
            "data index 126\n",
            "data index 127\n",
            "data index 128\n",
            "data index 129\n",
            "data index 130\n",
            "data index 131\n",
            "data index 132\n",
            "data index 133\n",
            "data index 134\n",
            "data index 135\n",
            "data index 136\n",
            "data index 137\n",
            "data index 138\n",
            "data index 139\n",
            "data index 140\n",
            "data index 141\n",
            "data index 142\n",
            "data index 143\n",
            "data index 144\n",
            "data index 145\n",
            "data index 146\n",
            "data index 147\n",
            "data index 148\n",
            "data index 149\n",
            "data index 150\n",
            "data index 151\n",
            "data index 152\n",
            "data index 153\n",
            "data index 154\n",
            "data index 155\n",
            "data index 156\n",
            "data index 157\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}